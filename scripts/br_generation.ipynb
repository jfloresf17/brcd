{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae595590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Silenciar logs de todo (incl. Earth Engine) y mostrar solo la barra\n",
    "import os, logging, warnings\n",
    "import multiprocessing as mp\n",
    "\n",
    "import geopandas as gpd\n",
    "import ee\n",
    "import rasterio as rio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.cloud_detection import (\n",
    "    get_s1_col,\n",
    "    get_s2cloudless_collection,\n",
    "    get_matching_s1_s2,\n",
    "    get_images,\n",
    ")\n",
    "\n",
    "# ---- parámetros de descarga\n",
    "clean_threshold = 0.9\n",
    "patch_size      = 64\n",
    "S2_BANDS        = [\"B4\",\"B3\",\"B2\",\"B8\",\"B11\",\"B12\"]\n",
    "S1_BANDS        = [\"VV\",\"VH\"]\n",
    "start_date      = \"2021-01-01\"\n",
    "end_date        = \"2021-12-31\"\n",
    "\n",
    "# ---- util MP\n",
    "def _get_mp_ctx():\n",
    "    try:\n",
    "        return mp.get_context(\"fork\") if \"fork\" in mp.get_all_start_methods() else mp.get_context(\"spawn\")\n",
    "    except Exception:\n",
    "        return mp\n",
    "\n",
    "# ---- inicializador por worker (sin prints)\n",
    "def _init_worker(quiet):\n",
    "    if quiet:\n",
    "        _silence_everything()\n",
    "    ee.Initialize(opt_url=None)\n",
    "\n",
    "# ---- función para silenciar librerías ruidosas\n",
    "def _silence_everything():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logging.captureWarnings(True)\n",
    "\n",
    "    # Nivel global\n",
    "    logging.basicConfig(level=logging.CRITICAL)\n",
    "    logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "    # Loggers específicos que suelen hablar en EE / Google\n",
    "    for name in [\n",
    "        \"ee\", \"earthengine\", \"earthengine-api\",\n",
    "        \"google\", \"googleapiclient\", \"googleapiclient.discovery\",\n",
    "        \"googleapiclient.http\", \"google.auth\", \"oauth2client\",\n",
    "        \"urllib3\", \"fiona\", \"rasterio\"\n",
    "    ]:\n",
    "        try:\n",
    "            logging.getLogger(name).setLevel(logging.CRITICAL)\n",
    "            logging.getLogger(name).propagate = False\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---- worker\n",
    "def process_centroid(task):\n",
    "    \"\"\"\n",
    "    task = (idx, lat, lon, out_s1_dir, out_s2_dir)\n",
    "    return: (True/False, idx)  (solo lo necesario para la barra)\n",
    "    \"\"\"\n",
    "    idx, lat, lon, out_s1_dir, out_s2_dir = task\n",
    "    name = str(idx).zfill(5)\n",
    "    try:\n",
    "        col_s2 = get_s2cloudless_collection(lat, lon, start_date, end_date,\n",
    "                                            clean_threshold, patch_size)\n",
    "        col_s1 = get_s1_col(lat, lon, start_date, end_date,\n",
    "                            patch_size, orbit_pass='DESCENDING')\n",
    "        m_s1, m_s2, _ = get_matching_s1_s2(col_s1, col_s2)\n",
    "        if not m_s1 or not m_s2:\n",
    "            return (False, idx)\n",
    "\n",
    "        m_s1 = m_s1[0]; m_s2 = m_s2[0]\n",
    "        s2_imgs, s2_prof = get_images(lat, lon, [m_s2], S2_BANDS, patch_size)\n",
    "        s1_imgs, s1_prof = get_images(lat, lon, [m_s1], S1_BANDS, patch_size)\n",
    "        s2_arr = s2_imgs[0][\"image\"]; s1_arr = s1_imgs[0][\"image\"]\n",
    "\n",
    "        os.makedirs(out_s1_dir, exist_ok=True)\n",
    "        os.makedirs(out_s2_dir, exist_ok=True)\n",
    "        s1_name = f\"s1_{name}.tif\"; s2_name = f\"s2_{name}.tif\"\n",
    "        with rio.open(os.path.join(out_s1_dir, s1_name), \"w\", **s1_prof) as dst:\n",
    "            dst.write(s1_arr)\n",
    "        with rio.open(os.path.join(out_s2_dir, s2_name), \"w\", **s2_prof) as dst:\n",
    "            dst.write(s2_arr)\n",
    "\n",
    "        return (True, idx)\n",
    "    except Exception:\n",
    "        return (False, idx)\n",
    "\n",
    "# ---- lanzador con SOLO la barra\n",
    "def run_downloads(centroids_gj_path, out_s1_dir, out_s2_dir,\n",
    "                  max_workers=24, chunksize=16, quiet=True):\n",
    "    if quiet:\n",
    "        _silence_everything()\n",
    "\n",
    "    gdf = gpd.read_file(centroids_gj_path).to_crs(epsg=4326)\n",
    "    tasks = [(i, row.geometry.y, row.geometry.x, out_s1_dir, out_s2_dir)\n",
    "             for i, (_, row) in enumerate(gdf.iterrows())]\n",
    "    total = len(tasks)\n",
    "\n",
    "    # inicializa EE una vez en el main\n",
    "    ee.Initialize()\n",
    "\n",
    "    n_workers = min(mp.cpu_count(), max_workers)\n",
    "    ctx = _get_mp_ctx()\n",
    "\n",
    "    ok = 0\n",
    "    err = 0\n",
    "\n",
    "    with ctx.Pool(processes=n_workers, initializer=_init_worker, initargs=(quiet,)) as pool:\n",
    "        with tqdm(total=total, unit=\"tile\", dynamic_ncols=True, desc=\"Descargando\") as pbar:\n",
    "            for success, _ in pool.imap_unordered(process_centroid, tasks, chunksize=chunksize):\n",
    "                if success:\n",
    "                    ok += 1\n",
    "                else:\n",
    "                    err += 1\n",
    "                pbar.update(1)\n",
    "                # postfix minimalista con conteos\n",
    "                pbar.set_postfix(ok=ok, err=err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26126b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_geojson = \"../dataset_tfm/spain.geojson\"\n",
    "out_s2_dir = \"../dataset_tfm/s2\"\n",
    "out_s1_dir = \"../dataset_tfm/s1\"\n",
    "run_downloads(centroids_geojson, out_s1_dir, out_s2_dir, max_workers=28, chunksize=16, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ed01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "DB_FOLDER = pathlib.Path(\"/home/tidop/masterIA/TFM_BRCD/dataset_tfm\")\n",
    "\n",
    "s2_files = sorted(list(DB_FOLDER.glob(\"**/s2/*.tif\")))\n",
    "s1_files = sorted(list(DB_FOLDER.glob(\"**/s1/*.tif\")))\n",
    "\n",
    "len(s2_files), len(s1_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, filter the files with NAN values and black areas\n",
    "s2_filtered = []\n",
    "s1_filtered = []\n",
    "\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "\n",
    "def is_valid_image(image_path):\n",
    "    with rio.open(image_path) as src:\n",
    "        data = src.read()\n",
    "        # Check for NaN values and if has values equal to zero across the bands\n",
    "        if (data == 0).all() or np.isnan(data).any():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for i, (s2_file, s1_file) in enumerate(zip(s2_files, s1_files)):\n",
    "    if is_valid_image(s2_file) and is_valid_image(s1_file):\n",
    "        s2_filtered.append(s2_file)\n",
    "        s1_filtered.append(s1_file)\n",
    "    else:\n",
    "        print(f\"Invalid file pair: {s2_file}, {s1_file}\")\n",
    "\n",
    "    print(f\"Processed {i+1}/{len(s2_files)}, valid pairs found: {len(s2_filtered)}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### shutil copy \n",
    "import shutil\n",
    "destination_folder = DB_FOLDER / \"filtered_data\"\n",
    "s1_destination_folder = destination_folder / \"s1\"\n",
    "s2_destination_folder = destination_folder / \"s2\"\n",
    "s1_destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "s2_destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "for s2_file, s1_file in zip(s2_filtered, s1_filtered):\n",
    "    shutil.copy(s2_file, s2_destination_folder)\n",
    "    shutil.copy(s1_file, s1_destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "ee.Initialize()\n",
    "n_workers = min(cpu_count(), 24)\n",
    "with Pool(processes=n_workers) as pool:\n",
    "    for i, res in enumerate(pool.imap_unordered(process_centroid, tasks), start=1):\n",
    "        if res is None:\n",
    "            continue\n",
    "        name, s1_fname, s2_fname = res\n",
    "        print(f\"[{i}/{len(tasks)}] {name} → S1: {s1_fname}, S2: {s2_fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63435eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.validation import make_valid\n",
    "from shapely.ops import unary_union\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===== Configuración =====\n",
    "AREA_THRESHOLD = 0.05  # 5 %\n",
    "ALL_TOUCHED = False     # Solo píxeles cuyo centro está dentro de la geometría\n",
    "CHUNKSIZE = 256         # Cuántas features procesa cada worker de golpe\n",
    "\n",
    "# ===== Funciones utilitarias =====\n",
    "def _get_mp_ctx():\n",
    "    return mp.get_context('fork') if 'fork' in mp.get_all_start_methods() else mp.get_context('spawn')\n",
    "\n",
    "def _clean_geom(geom):\n",
    "    if geom.is_empty:\n",
    "        return None\n",
    "    g = make_valid(geom)\n",
    "    if g.is_empty:\n",
    "        return None\n",
    "    if g.geom_type == \"GeometryCollection\":\n",
    "        polys = [p for p in g.geoms if p.geom_type in (\"Polygon\", \"MultiPolygon\")]\n",
    "        if not polys:\n",
    "            return None\n",
    "        g = unary_union(polys)\n",
    "    return g if g.is_valid and not g.is_empty else None\n",
    "\n",
    "# ===== Variables globales para multiprocessing =====\n",
    "_RDS = None\n",
    "_PIXEL_AREA = None\n",
    "\n",
    "def _init_worker(raster_path):\n",
    "    global _RDS, _PIXEL_AREA\n",
    "    _RDS = rasterio.open(raster_path)\n",
    "    # Calcula área de un píxel en m²\n",
    "    px_w = abs(_RDS.transform.a)\n",
    "    px_h = abs(_RDS.transform.e)\n",
    "    _PIXEL_AREA = px_w * px_h\n",
    "\n",
    "def _process_feature(feat):\n",
    "    global _RDS, _PIXEL_AREA\n",
    "    shp = _clean_geom(feat.geometry)\n",
    "    if shp is None:\n",
    "        return None\n",
    "\n",
    "    cell_area = shp.area\n",
    "    if cell_area <= 0 or not math.isfinite(cell_area):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        window = from_bounds(*shp.bounds, transform=_RDS.transform)\n",
    "        arr = _RDS.read(1, window=window, boundless=True, masked=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if arr.size == 0:\n",
    "        return None\n",
    "\n",
    "    win_transform = rasterio.windows.transform(window, _RDS.transform)\n",
    "    mask_inside = geometry_mask(\n",
    "        [mapping(shp)],\n",
    "        out_shape=arr.shape,\n",
    "        transform=win_transform,\n",
    "        invert=True,\n",
    "        all_touched=ALL_TOUCHED\n",
    "    )\n",
    "\n",
    "    valid_pixels = np.logical_and(mask_inside, ~arr.mask) if hasattr(arr, \"mask\") else mask_inside\n",
    "    pix1 = np.logical_and(valid_pixels, arr == 1)\n",
    "    built_area = pix1.sum() * _PIXEL_AREA\n",
    "\n",
    "    if (built_area / cell_area) >= AREA_THRESHOLD:\n",
    "        return feat\n",
    "    return None\n",
    "\n",
    "# ===== Función principal =====\n",
    "def filtrar_grilla_por_raster(vector_path, raster_path, output_path, processes=None):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Leer raster\n",
    "    with rasterio.open(raster_path) as rds:\n",
    "        raster_crs = rds.crs\n",
    "\n",
    "    # Leer vector\n",
    "    gdf = gpd.read_file(vector_path)\n",
    "\n",
    "    # Reproyectar si es necesario\n",
    "    if gdf.crs != raster_crs:\n",
    "        print(f\"Reproyectando vector de {gdf.crs} a {raster_crs}\")\n",
    "        gdf = gdf.to_crs(raster_crs)\n",
    "\n",
    "    if processes is None:\n",
    "        processes = max(1, mp.cpu_count() - 1)\n",
    "\n",
    "    ctx = _get_mp_ctx()\n",
    "    results = []\n",
    "    with ctx.Pool(processes=processes, initializer=_init_worker, initargs=(raster_path,)) as pool:\n",
    "        for res in tqdm(pool.imap_unordered(_process_feature, [row for _, row in gdf.iterrows()], chunksize=CHUNKSIZE),\n",
    "                        total=len(gdf), unit=\"feat\", dynamic_ncols=True):\n",
    "            if res is not None:\n",
    "                results.append(res)\n",
    "\n",
    "    # Guardar resultado\n",
    "    out_gdf = gpd.GeoDataFrame(results, columns=gdf.columns, crs=raster_crs)\n",
    "    out_gdf.to_file(output_path, driver=\"GeoJSON\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"✅ Listo. Total: {len(gdf)} | Conservados: {len(out_gdf)} (≥ {AREA_THRESHOLD*100:.1f} % área)\")\n",
    "    print(f\"⏱️ {elapsed:.1f}s | {len(gdf)/elapsed:,.0f} feat/s\")\n",
    "    print(f\"➡️ {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62470b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_in = \"../brcd_data/whole_spain_grided_builtup.geojson\"\n",
    "raster_in = \"../brcd_data/Built-up_epsg3035_2.5m.tif\"\n",
    "salida = \"../brcd_data/spain_filtrada.geojson\"\n",
    "filtrar_grilla_por_raster(vector_in, raster_in, salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter if has 0 or NAN both S2 and S1 file\n",
    "import pathlib\n",
    "s2_files = sorted(list(pathlib.Path(out_s2).glob(\"*.tif\")))\n",
    "s1_files = sorted(list(pathlib.Path(out_s1).glob(\"*.tif\")))\n",
    "\n",
    "for i, (s2_file, s1_file) in enumerate(zip(s2_files, s1_files)):\n",
    "    with rio.open(s2_file) as src:\n",
    "        s2_data = src.read()\n",
    "        if (s2_data == 0).all() or (s2_data != s2_data).all():\n",
    "            print(f\"[FILTER] Removing {s2_file} due to all zeros or NaNs\")\n",
    "            s2_file.unlink()\n",
    "            continue\n",
    "\n",
    "    with rio.open(s1_file) as src:\n",
    "        s1_data = src.read()\n",
    "        if (s1_data == 0).all() or (s1_data != s1_data).all():\n",
    "            print(f\"[FILTER] Removing {s1_file} due to all zeros or NaNs\")\n",
    "            s1_file.unlink()\n",
    "            continue\n",
    "\n",
    "print(f\"[INFO] Filtered {len(s2_files) - len(list(pathlib.Path(out_s2).glob('*.tif')))} S2 files and {len(s1_files) - len(list(pathlib.Path(out_s1).glob('*.tif')))} S1 files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad595b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download building\n",
    "import pathlib\n",
    "import rasterio as rio\n",
    "\n",
    "DB_FOLDER = pathlib.Path(\"../brcd\")\n",
    "\n",
    "building_files = sorted(list((DB_FOLDER / \"building\").glob(\"*.tif\")))\n",
    "road_files = sorted(list((DB_FOLDER / \"road\").glob(\"*.tif\")))\n",
    "\n",
    "## Filter if has less than 2.5% of non-zero pixels (both building and road)\n",
    "\n",
    "building_filtered_files = []\n",
    "road_filtered_files = []\n",
    "\n",
    "for i, (building_file, road_file) in enumerate(zip(building_files, road_files)):\n",
    "    with rio.open(building_file) as src1, rio.open(road_file) as src2:\n",
    "        building_data = src1.read()\n",
    "        road_data = src2.read()\n",
    "\n",
    "        # Calculate non-zero pixel percentages\n",
    "        building_non_zero = (building_data != 0).sum() / building_data.size\n",
    "        road_non_zero = (road_data != 0).sum() / road_data.size\n",
    "\n",
    "        if building_non_zero >= 0.025 and road_non_zero >= 0.025:\n",
    "            building_filtered_files.append(building_file)\n",
    "            road_filtered_files.append(road_file)\n",
    "\n",
    "            print(f\"[{i+1}/{len(building_files)}] Keeping {building_file} and {road_file} with {building_non_zero:.2%} building and {road_non_zero:.2%} road non-zero pixels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e09e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(building_filtered_files), len(road_filtered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "## Create new folders with\n",
    "NEW_FOLDER = pathlib.Path(\"../dataset_brcd\")\n",
    "NEW_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "S1_FOLDER = NEW_FOLDER / \"s1\"\n",
    "S2_FOLDER = NEW_FOLDER / \"s2\"\n",
    "BUILDING_FOLDER = NEW_FOLDER / \"building\"   \n",
    "ROAD_FOLDER = NEW_FOLDER / \"road\"\n",
    "\n",
    "S1_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "S2_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "BUILDING_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "ROAD_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, (building_file, road_file) in enumerate(zip(building_filtered_files, road_filtered_files)):  \n",
    "    s1_file = building_file.parent.parent / \"s1\" / building_file.name.replace(\"building\", \"s1\")\n",
    "    s2_file = building_file.parent.parent / \"s2\" / building_file.name.replace(\"building\", \"s2\")\n",
    "\n",
    "    s1_dest = S1_FOLDER / s1_file.name\n",
    "    s2_dest = S2_FOLDER / s2_file.name   \n",
    "    building_dest = BUILDING_FOLDER / building_file.name\n",
    "    road_dest = ROAD_FOLDER / road_file.name\n",
    "\n",
    "    if s1_file.exists() and s2_file.exists():\n",
    "        shutil.copy(s1_file, s1_dest)\n",
    "        shutil.copy(s2_file, s2_dest)\n",
    "        shutil.copy(building_file, building_dest)\n",
    "        shutil.copy(road_file, road_dest)\n",
    "        print(f\"[{i+1}/{len(building_filtered_files)}] Copied {s1_file.name}, {s2_file.name}, {building_file.name}, and {road_file.name} to new dataset folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unlink files with NAN VALUES in s2 or s1 files\n",
    "import pathlib\n",
    "s2_files = sorted(list(pathlib.Path(S2_FOLDER).glob(\"*.tif\")))\n",
    "s1_files = sorted(list(pathlib.Path(S1_FOLDER).glob(\"*.tif\")))\n",
    "\n",
    "non_nan_s2_files = []\n",
    "non_nan_s1_files = []\n",
    "for i, (s2_file, s1_file) in enumerate(zip(s2_files, s1_files)):\n",
    "\n",
    "    with rio.open(s2_file) as src1, rio.open(s1_file) as src2:\n",
    "        s2_data = src1.read()\n",
    "        s1_data = src2.read()\n",
    "\n",
    "        # Check for NaN values\n",
    "        if np.isnan(s2_data).any() or np.isnan(s1_data).any():\n",
    "           continue\n",
    "        else:\n",
    "            non_nan_s2_files.append(s2_file)\n",
    "            non_nan_s1_files.append(s1_file)\n",
    "            print(f\"[{i+1}/{len(s2_files)}] Keeping {s2_file} and {s1_file} with no NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b200f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_nan_s2_files), len(non_nan_s1_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"S2 Mean: {s2_mean/10_000}, S2 Std: {s2_std/10_000}\")\n",
    "print(f\"S1 Mean: {s1_mean}, S1 Std: {s1_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "def compute_dataset_class_percentages(tif_paths, building_value=1, road_value=2):\n",
    "    \"\"\"\n",
    "    Calcula el porcentaje promedio de píxeles de edificios y carreteras\n",
    "    en un conjunto de archivos TIFF de etiquetas.\n",
    "\n",
    "    Args:\n",
    "        tif_paths (list[str]): Lista de rutas a archivos TIFF de máscara.\n",
    "        building_value (int): Valor de pixel que codifica edificios en la máscara.\n",
    "        road_value (int): Valor de pixel que codifica carreteras en la máscara.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_building, avg_road) porcentajes promedio sobre todos los píxeles.\n",
    "    \"\"\"\n",
    "    total_pixels = 0\n",
    "    total_class = 0\n",
    "\n",
    "    for path in tif_paths:\n",
    "        with rio.open(path) as src:\n",
    "            mask = src.read(1)  # lee la primera banda de la máscara\n",
    "        total_pixels += mask.size\n",
    "        total_class += np.count_nonzero(mask == 1)\n",
    "\n",
    "    avg_class = total_class / total_pixels\n",
    "    return avg_class\n",
    "\n",
    "# Ejemplo de llamada:\n",
    "BUILDING_FOLDER = \"../dataset_brcd/road\"\n",
    "tif_list = sorted(list(pathlib.Path(BUILDING_FOLDER).glob(\"*.tif\")))\n",
    "avg_c = compute_dataset_class_percentages(tif_list)\n",
    "print(f\"Promedio clase: {avg_c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a313e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from overturemaps import core\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "import pyproj\n",
    "from shapely.geometry import box\n",
    "from multiprocessing import Pool\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in area\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Buffers por clase\n",
    "buffer_dict = {\n",
    "    'motorway': 3.5, 'motorway_link': 2.5,\n",
    "    'trunk': 3.5, 'trunk_link': 2.5,\n",
    "    'primary': 3.5, 'primary_link': 2.5,\n",
    "    'secondary': 3.5, 'secondary_link': 2.5,\n",
    "    'tertiary': 3.5, 'tertiary_link': 2.5,\n",
    "    'residential': 2.5, 'living_street': 2.5,\n",
    "    'unclassified': 1.25, 'pedestrian': 1.25,\n",
    "    'footway': 1.25, 'cycleway': 1.25,\n",
    "    'service': 2.5, 'steps': 1.25,\n",
    "    'path': 1.25, 'track': 1.25, 'bridleway': 1.25,\n",
    "    'busway': 2.5, 'unknown': 1.25\n",
    "}\n",
    "\n",
    "def create_masks(s2_path, out_tif_building, out_tif_road, pixel_size=2.5, out_shape=(256, 256)):\n",
    "    \"\"\"\n",
    "    Crea las máscaras (building/road). Devuelve dict: {'status': 'ok'|'skipped'|'error', 'msg': str}\n",
    "    \"\"\"\n",
    "    s2_path = Path(s2_path); out_tif_building = Path(out_tif_building); out_tif_road = Path(out_tif_road)\n",
    "    try:\n",
    "        # Omitir si ambos ya existen\n",
    "        if out_tif_building.exists() and out_tif_road.exists():\n",
    "            return {\"status\": \"skipped\", \"msg\": f\"{out_tif_building.name} y {out_tif_road.name} existen, se omite.\"}\n",
    "\n",
    "        with rio.open(s2_path) as src:\n",
    "            minx, miny, maxx, maxy = src.bounds\n",
    "            transformer = pyproj.Transformer.from_crs(src.crs, \"EPSG:4326\", always_xy=True)\n",
    "            minx_wgs, miny_wgs = transformer.transform(minx, miny)\n",
    "            maxx_wgs, maxy_wgs = transformer.transform(maxx, maxy)\n",
    "            bbox_wgs84 = (minx_wgs, miny_wgs, maxx_wgs, maxy_wgs)\n",
    "\n",
    "            # Overture\n",
    "            building_gdf = core.geodataframe(\"building\", bbox=bbox_wgs84); building_gdf.crs = \"EPSG:4326\"\n",
    "            road_gdf     = core.geodataframe(\"segment\",  bbox=bbox_wgs84); road_gdf.crs     = \"EPSG:4326\"\n",
    "\n",
    "            # Reproyección + recorte\n",
    "            bbox_geom = box(minx, miny, maxx, maxy)\n",
    "            building_gdf = building_gdf.to_crs(src.crs)\n",
    "            building_gdf = building_gdf[building_gdf.geometry.intersects(bbox_geom)]\n",
    "            building_gdf = building_gdf[building_gdf.geometry.area > 100]  # filtra edificios pequeños\n",
    "\n",
    "            road_gdf = road_gdf.to_crs(src.crs)\n",
    "            road_gdf = road_gdf[road_gdf.geometry.intersects(bbox_geom)]\n",
    "            if 'class' in road_gdf.columns:\n",
    "                road_gdf['buffer'] = road_gdf['class'].map(buffer_dict).fillna(1.25)\n",
    "            else:\n",
    "                road_gdf['buffer'] = 1.25\n",
    "            road_gdf['geometry'] = road_gdf.apply(lambda r: r['geometry'].buffer(r['buffer']), axis=1)\n",
    "\n",
    "            # Transform y rasterización\n",
    "            transform = rio.Affine(pixel_size, 0, minx, 0, -pixel_size, maxy)\n",
    "            building_raster = rasterize([(g, 1) for g in building_gdf.geometry], out_shape=out_shape, transform=transform, fill=0, dtype='uint8')\n",
    "            road_raster     = rasterize([(g, 1) for g in road_gdf.geometry],     out_shape=out_shape, transform=transform, fill=0, dtype='uint8')\n",
    "\n",
    "            meta = dict(driver='GTiff', height=out_shape[0], width=out_shape[1], count=1, dtype='uint8', crs=src.crs, transform=transform)\n",
    "            out_tif_building.parent.mkdir(parents=True, exist_ok=True)\n",
    "            out_tif_road.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with rio.open(out_tif_building, 'w', **meta) as dst: dst.write(building_raster, 1)\n",
    "            with rio.open(out_tif_road, 'w', **meta) as dst:     dst.write(road_raster, 1)\n",
    "\n",
    "        return {\"status\": \"ok\", \"msg\": f\"Procesado {s2_path.name}\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"msg\": f\"Error en {s2_path}: {e}\"}\n",
    "\n",
    "def _worker(args):\n",
    "    return create_masks(*args)\n",
    "\n",
    "def split_existing(pairs, show_progress=True):\n",
    "    \"\"\"\n",
    "    Separa pares en existentes (ambos outputs ya están) y a_procesar.\n",
    "    Devuelve existing_pairs, todo_pairs y muestra una barra de verificación.\n",
    "    \"\"\"\n",
    "    existing, todo = [], []\n",
    "    if show_progress:\n",
    "        pbar = tqdm(pairs, desc=\"Verificando existentes\", unit=\"par\")\n",
    "    else:\n",
    "        pbar = pairs\n",
    "    count_exist = 0\n",
    "    for item in pbar:\n",
    "        s2, out_b, out_r = item\n",
    "        if Path(out_b).exists() and Path(out_r).exists():\n",
    "            existing.append(item); count_exist += 1\n",
    "        else:\n",
    "            todo.append(item)\n",
    "        if show_progress:\n",
    "            tqdm.write(\"\", end=\"\")  # mantener limpia la barra en algunos entornos\n",
    "            pbar.set_postfix(existentes=count_exist, por_hacer=len(todo), refresh=True)\n",
    "    if show_progress and hasattr(pbar, \"close\"):\n",
    "        pbar.close()\n",
    "    return existing, todo\n",
    "\n",
    "def parallel_create_masks(\n",
    "    pairs,\n",
    "    processes=16,\n",
    "    pixel_size=2.5,\n",
    "    out_shape=(256, 256),\n",
    "    show_progress=True,\n",
    "    desc=\"Procesando (descarga + rasterización)\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta con:\n",
    "      - Barra 1: conteo de existentes (skip)\n",
    "      - Barra 2: procesamiento de faltantes con ETA, errores y descargados\n",
    "    \"\"\"\n",
    "    # Barra 1: contar existentes\n",
    "    existing_pairs, todo_pairs = split_existing(pairs, show_progress=show_progress)\n",
    "\n",
    "    # Crear resultados \"skipped\" para existentes (para resumen final)\n",
    "    skipped_results = [\n",
    "        {\"status\": \"skipped\", \"msg\": f\"{Path(b).name} y {Path(r).name} ya existen\"}\n",
    "        for _, b, r in existing_pairs\n",
    "    ]\n",
    "\n",
    "    # Si no hay nada que hacer, mostramos un mini-resumen y salimos\n",
    "    if not todo_pairs:\n",
    "        if show_progress:\n",
    "            tqdm.write(f\"Todo listo: {len(existing_pairs)} ya existentes, 0 por procesar.\")\n",
    "        return skipped_results\n",
    "\n",
    "    # Preparar argumentos con parámetros fijos\n",
    "    todo_args = [(*p, pixel_size, out_shape) for p in todo_pairs]\n",
    "\n",
    "    done = 0\n",
    "    errors = 0\n",
    "    results = []\n",
    "\n",
    "    if processes is None or processes <= 1:\n",
    "        # Secuencial\n",
    "        pbar = tqdm(total=len(todo_args), disable=not show_progress, desc=desc, unit=\"par\")\n",
    "        for args in todo_args:\n",
    "            res = create_masks(*args)\n",
    "            results.append(res)\n",
    "            if res[\"status\"] == \"ok\":\n",
    "                done += 1\n",
    "            elif res[\"status\"] == \"error\":\n",
    "                errors += 1\n",
    "            pbar.set_postfix(descargados=done, errores=errors, refresh=True)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        return skipped_results + results\n",
    "\n",
    "    # Paralelo\n",
    "    with Pool(processes=processes) as pool:\n",
    "        pbar = tqdm(total=len(todo_args), disable=not show_progress, desc=desc, unit=\"par\")\n",
    "        for res in pool.imap_unordered(_worker, todo_args, chunksize=1):\n",
    "            results.append(res)\n",
    "            if res[\"status\"] == \"ok\":\n",
    "                done += 1\n",
    "            elif res[\"status\"] == \"error\":\n",
    "                errors += 1\n",
    "            pbar.set_postfix(descargados=done, errores=errors, last=res.get(\"msg\", \"\")[:60], refresh=True)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    return skipped_results + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d233c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179cdae446074e40a81f6247e12c6436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Verificando existentes:   0%|          | 0/39877 [00:00<?, ?par/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f30a81c01344beb1497df4ac798032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando (descarga + rasterización):   0%|          | 0/30089 [00:00<?, ?par/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "DB_FOLDER = Path(\"../dataset_tfm/filtered_data\")\n",
    "S2_DIR = DB_FOLDER / \"s2\"\n",
    "BUILD_DIR = DB_FOLDER / \"building\"; BUILD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ROAD_DIR  = DB_FOLDER / \"road\";     ROAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "s2_files = sorted(S2_DIR.glob(\"*.tif\"))\n",
    "pairs = []\n",
    "for s2 in s2_files:\n",
    "    token_parts = s2.stem.split(\"_\")\n",
    "    token = token_parts[1] if len(token_parts) > 1 else s2.stem\n",
    "    out_build = BUILD_DIR / f\"building_{token}.tif\"\n",
    "    out_road  = ROAD_DIR  / f\"road_{token}.tif\"\n",
    "    pairs.append((str(s2), str(out_build), str(out_road)))\n",
    "\n",
    "results = parallel_create_masks(\n",
    "    pairs,\n",
    "    processes=min(24, cpu_count()),\n",
    "    pixel_size=2.5,\n",
    "    out_shape=(256, 256),\n",
    "    show_progress=True,\n",
    "    desc=\"Procesando (descarga + rasterización)\"\n",
    ")\n",
    "\n",
    "# Resumen\n",
    "ok = sum(1 for r in results if r and r.get(\"status\") == \"ok\")\n",
    "sk = sum(1 for r in results if r and r.get(\"status\") == \"skipped\")\n",
    "er = sum(1 for r in results if r and r.get(\"status\") == \"error\")\n",
    "print(f\"Resumen → Descargados/Procesados: {ok} | Ya existentes: {sk} | Errores: {er}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb489d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12019355,0.10635965,0.08203992,0.23945379,0.2044379,,0.15833034], [,-9.21341266,-16.8054372,]]\n",
      "[[0.06304411,0.05342577,0.04976318,0.081813,,,0.05458699,0.05351668], [4.07210856,4.13933744]]\n"
     ]
    }
   ],
   "source": [
    "# Get the mean and std for the s2 and s1 bands\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "def calculate_mean_std(files):\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for file in files:\n",
    "        with rio.open(file) as src:\n",
    "            data = src.read()\n",
    "            means.append(np.mean(data, axis=(1, 2)))\n",
    "            stds.append(np.std(data, axis=(1, 2)))\n",
    "    \n",
    "    return np.mean(means, axis=0), np.mean(stds, axis=0)\n",
    "\n",
    "import pathlib\n",
    "S2_FOLDER = \"../data/s2\"\n",
    "S1_FOLDER = \"../data/s1\"\n",
    "\n",
    "s2_files = sorted(list(pathlib.Path(S2_FOLDER).glob(\"*.tif\")))\n",
    "s1_files = sorted(list(pathlib.Path(S1_FOLDER).glob(\"*.tif\")))\n",
    "\n",
    "s2_mean, s2_std = calculate_mean_std(s2_files)\n",
    "s1_mean, s1_std = calculate_mean_std(s1_files)\n",
    "\n",
    "print(f\"[{str(s2_mean / 10_000).replace(' ', ',')}, {str(s1_mean).replace(' ', ',')}]\")\n",
    "print(f\"[{str(s2_std / 10_000).replace(' ', ',')}, {str(s1_std).replace(' ', ',')}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee008ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Probar el ploteo de etiquetas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
